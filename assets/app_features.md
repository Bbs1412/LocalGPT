1. Runs Locally with Streamlit.
2. Any model from Ollama can be used. 
3. Set any locally installed model as default (code).
4. Supports Streaming Responses with Live Preview.
5. Support attachments like Images (currently only images are supported).
6. Supports Threads (Chat Archives).
7. Can rename, create and delete threads.
8. Threads are listed as per last used.
9. Remembers the last model used with thread, which is auto-loaded next time.
10. Can switch between models within same thread.
11. Running models can be checked and stopped.
12. Supports "Thinking" models.